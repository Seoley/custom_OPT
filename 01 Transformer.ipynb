{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2a43fb7",
   "metadata": {},
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5e3d299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00000-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00001-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00002-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00003-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00004-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00005-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00006-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00007-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00008-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00009-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00010-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00011-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00012-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00013-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00014-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00015-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00016-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00017-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00018-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00019-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00020-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00021-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00022-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00023-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00024-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00025-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00026-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00027-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00028-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00029-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00030-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00031-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00032-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00033-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00034-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00035-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00036-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00037-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00038-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00039-of-00041.arrow'}, {'filename': '/home/zeus/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia-train-00040-of-00041.arrow'}]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\")\n",
    "\n",
    "# 캐시된 경로 확인\n",
    "print(dataset.cache_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8354988b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3098, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3153, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3362, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3607, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_17909/344259398.py\", line 1, in <module>\n",
      "    import spacy\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/spacy/__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/spacy/errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/spacy/compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/thinc/__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/thinc/config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/thinc/types.py\", line 27, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/thinc/compat.py\", line 35, in <module>\n",
      "    import torch\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_en = spacy.load('en_core_web_sm') # 영어 토큰화(tokenization)\n",
    "spacy_de = spacy.load('de_core_news_sm') # 독일어 토큰화(tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "603fac60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인덱스 0: I\n",
      "인덱스 1: am\n",
      "인덱스 2: a\n",
      "인덱스 3: graduate\n",
      "인덱스 4: student\n",
      "인덱스 5: .\n"
     ]
    }
   ],
   "source": [
    "# 간단히 토큰화(tokenization) 기능 써보기\n",
    "tokenized = spacy_en.tokenizer(\"I am a graduate student.\")\n",
    "\n",
    "for i, token in enumerate(tokenized):\n",
    "    print(f\"인덱스 {i}: {token.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a9cbeb",
   "metadata": {},
   "source": [
    "- 영어 및 독일어 토큰화 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a269764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 독일어(Deutsch) 문장을 토큰화 하는 함수 (순서를 뒤집지 않음)\n",
    "def tokenize_de(text):\n",
    "    return [token.text for token in spacy_de.tokenizer(text)]\n",
    "\n",
    "# 영어(English) 문장을 토큰화 하는 함수\n",
    "def tokenize_en(text):\n",
    "    return [token.text for token in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ce8d86",
   "metadata": {},
   "source": [
    "- 데이터셋 로드\n",
    "  - huggingface 데이터 다운로드 중, timeout 에러가 발생하여 강제로 60초로 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b70adedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import huggingface_hub\n",
    "\n",
    "# 타임아웃을 60초로 늘리기\n",
    "huggingface_hub.constants.HF_HUB_HTTP_TIMEOUT = 60\n",
    "\n",
    "# 다시 시도\n",
    "dataset = load_dataset(\"bentrevett/multi30k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d488c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "valid_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ded71de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "218e373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3855bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['en', 'de'],\n",
       "    num_rows: 29000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a1a63af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터셋(training dataset) 크기: 29000개\n",
      "평가 데이터셋(validation dataset) 크기: 1014개\n",
      "테스트 데이터셋(testing dataset) 크기: 1000개\n"
     ]
    }
   ],
   "source": [
    "print(f\"학습 데이터셋(training dataset) 크기: {len(train_dataset)}개\")\n",
    "print(f\"평가 데이터셋(validation dataset) 크기: {len(valid_dataset)}개\")\n",
    "print(f\"테스트 데이터셋(testing dataset) 크기: {len(test_dataset)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb53b9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(SRC): 7853\n",
      "len(TRG): 5893\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer 준비 (spacy 설치 필요)\n",
    "tokenizer_de = get_tokenizer(\"spacy\", language=\"de_core_news_sm\")\n",
    "tokenizer_en = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "\n",
    "# 특수 토큰 정의\n",
    "specials = [\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"]\n",
    "\n",
    "# Vocab 생성용 generator 함수 정의\n",
    "def yield_tokens(data, lang, tokenizer):\n",
    "    for example in data:\n",
    "        yield tokenizer(example[lang].lower())\n",
    "\n",
    "# SRC vocab (독일어)\n",
    "vocab_src = build_vocab_from_iterator(\n",
    "    yield_tokens(train_dataset, \"de\", tokenizer_de),\n",
    "    specials=specials,\n",
    "    min_freq=2\n",
    ")\n",
    "vocab_src.set_default_index(vocab_src[\"<unk>\"])\n",
    "\n",
    "# TRG vocab (영어)\n",
    "vocab_trg = build_vocab_from_iterator(\n",
    "    yield_tokens(train_dataset, \"en\", tokenizer_en),\n",
    "    specials=specials,\n",
    "    min_freq=2\n",
    ")\n",
    "vocab_trg.set_default_index(vocab_trg[\"<unk>\"])\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"len(SRC): {len(vocab_src)}\")\n",
    "print(f\"len(TRG): {len(vocab_trg)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b20a88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext._torchtext.Vocab at 0x7f6e1e985470>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_trg.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a65e8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4111\n",
      "1752\n"
     ]
    }
   ],
   "source": [
    "print(vocab_trg.vocab[\"abcabc\"]) # 없는 단어: 0\n",
    "print(vocab_trg.vocab[\"<pad>\"]) # 패딩(padding): 1\n",
    "print(vocab_trg.vocab[\"<sos>\"]) # <sos>: 2\n",
    "print(vocab_trg.vocab[\"<eos>\"]) # <eos>: 3\n",
    "print(vocab_trg.vocab[\"hello\"])\n",
    "print(vocab_trg.vocab[\"world\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a44e64f",
   "metadata": {},
   "source": [
    "- 데이터 로더 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "917cadf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7be9e0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def process(example, src_tokenizer, trg_tokenizer, vocab_src, vocab_trg):\n",
    "    src_tokens = [\"<sos>\"] + src_tokenizer(example['de'].lower()) + [\"<eos>\"]\n",
    "    trg_tokens = [\"<sos>\"] + trg_tokenizer(example['en'].lower()) + [\"<eos>\"]\n",
    "\n",
    "    src_indices = [vocab_src[token] for token in src_tokens]\n",
    "    trg_indices = [vocab_trg[token] for token in trg_tokens]\n",
    "\n",
    "    return torch.tensor(src_indices), torch.tensor(trg_indices)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*[\n",
    "        process(example, tokenizer_de, tokenizer_en, vocab_src, vocab_trg)\n",
    "        for example in batch\n",
    "    ])\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=vocab_src[\"<pad>\"], batch_first=True)\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=vocab_trg[\"<pad>\"], batch_first=True)\n",
    "\n",
    "    return src_batch.to(device), trg_batch.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e73b6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*[\n",
    "        process(example, tokenizer_de, tokenizer_en, vocab_src, vocab_trg)\n",
    "        for example in batch\n",
    "    ])\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=vocab_src[\"<pad>\"], batch_first=True)\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=vocab_trg[\"<pad>\"], batch_first=True)\n",
    "\n",
    "    return src_batch.to(device), trg_batch.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5aab30e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2dc50f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 배치 크기: torch.Size([128, 27])\n",
      "인덱스 0: 2\n",
      "인덱스 1: 18\n",
      "인덱스 2: 26\n",
      "인덱스 3: 253\n",
      "인덱스 4: 30\n",
      "인덱스 5: 84\n",
      "인덱스 6: 20\n",
      "인덱스 7: 88\n",
      "인덱스 8: 7\n",
      "인덱스 9: 15\n",
      "인덱스 10: 110\n",
      "인덱스 11: 7647\n",
      "인덱스 12: 3171\n",
      "인덱스 13: 4\n",
      "인덱스 14: 3\n",
      "인덱스 15: 1\n",
      "인덱스 16: 1\n",
      "인덱스 17: 1\n",
      "인덱스 18: 1\n",
      "인덱스 19: 1\n",
      "인덱스 20: 1\n",
      "인덱스 21: 1\n",
      "인덱스 22: 1\n",
      "인덱스 23: 1\n",
      "인덱스 24: 1\n",
      "인덱스 25: 1\n",
      "인덱스 26: 1\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_loader):\n",
    "    src = batch[0]\n",
    "    trg = batch[1]\n",
    "\n",
    "    print(f\"첫 번째 배치 크기: {src.shape}\")\n",
    "\n",
    "    # 현재 배치에 있는 하나의 문장에 포함된 정보 출력\n",
    "    for i in range(src.shape[1]):\n",
    "        print(f\"인덱스 {i}: {src[0][i].item()}\") # 여기에서는 [Seq_num, Seq_len]\n",
    "\n",
    "    # 첫 번째 배치만 확인\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b6a473",
   "metadata": {},
   "source": [
    "### Multi-head attention\n",
    "- 어텐션 입력값\n",
    "  - q: 쿼리\n",
    "  - k: 키\n",
    "  - v: 값\n",
    "- 하이퍼 파라미터\n",
    "  - hidden_dim: 하나의 단어에 대한 임베딩 차원\n",
    "  - n_heads: 헤드의 갯수(scaled dot-product attention)\n",
    "  - dropout_ratio: 드롭아웃의 비율"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5c052f",
   "metadata": {},
   "source": [
    "### 구현해야하는 것\n",
    "- Multi-head attention\n",
    "  - hidden_dim = head_dim * n_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f727527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "547430e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, dropout_ratio, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hidden_dim // n_heads\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "\n",
    "        self.q_linear = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.k_linear = nn.Linear(hidden_dim, hidden_dim, bias=False)   \n",
    "        self.v_linear = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.out_linear = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        batch_size, seq_len, hidden_dim = q.shape\n",
    "\n",
    "        # q: [batch_size, query_len, hidden_dim]\n",
    "        # k: [batch_size, key_len, hidden_dim]\n",
    "        # v: [batch_size, value_len, hidden_dim]\n",
    "        # query_len, key_len, value_len은 모두 seq_len과 같음\n",
    "\n",
    "        Q = self.q_linear(q) \n",
    "        K = self.k_linear(k)\n",
    "        V = self.v_linear(v)\n",
    "\n",
    "        # [batch_size, seq_len, hidden_dim] -> [batch_size, n_heads, seq_len, head_dim] 으로 조정 필요\n",
    "\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3) \n",
    "\n",
    "        # Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V\n",
    "\n",
    "        d_k = torch.sqrt(torch.FloatTensor([self.head_dim])).to(self.device)\n",
    "\n",
    "        # energy: [batch_size, n_heads, query_len, key_len]\n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / d_k\n",
    "\n",
    "        # 마스크(mask)를 사용하는 경우\n",
    "        if mask is not None:\n",
    "            # 마스크(mask) 값이 0인 부분을 -1e10으로 채우기\n",
    "            energy = energy.masked_fill(mask==0, -1e10)\n",
    "\n",
    "        # attention: [batch_size, n_heads, seq_len, head_dim]\n",
    "        attention_scores = torch.softmax(energy, dim=-1)\n",
    "        attention = torch.matmul(attention_scores, V)\n",
    "\n",
    "        # attention: [batch_size, n_heads, seq_len, head_dim] -> x: [batch_size, seq_len, n_heads, head_dim]\n",
    "        x = attention.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "        # x: [batch_size, seq_len, hidden_dim]\n",
    "        x = x.view(batch_size, -1, self.hidden_dim)\n",
    "\n",
    "        return x, attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "383868e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, pf_dim, dropout_ratio=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(hidden_dim, pf_dim, bias=False)\n",
    "        self.linear_2 = nn.Linear(pf_dim, hidden_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear_2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b69948",
   "metadata": {},
   "source": [
    "### Encoder 아키텍쳐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "737d6811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
    "        super().__init__()\n",
    "\n",
    "        # input and ouput have same dimension\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.self_attention = MultiheadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
    "        self.feedforward = FeedForwardLayer(hidden_dim, pf_dim, dropout_ratio)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "\n",
    "        _src = self.feedforward(src)\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f58b00cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length=100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hidden_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hidden_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        # src: [batch_size, src_len]\n",
    "        # src_mask: [batch_size, src_len]\n",
    "\n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "\n",
    "        # pos: [batch_size, src_lne]\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c544ec7",
   "metadata": {},
   "source": [
    "### 디코더 아키텍쳐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95fdcafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention_1 = MultiheadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
    "        self.self_attn_layer_norm_1 = nn.LayerNorm(hidden_dim)\n",
    "        self.self_attention_2 = MultiheadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
    "        self.self_attn_layer_norm_2 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.feedforward = FeedForwardLayer(hidden_dim, pf_dim, dropout_ratio)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        _trg, _ = self.self_attention_1(trg, trg, trg, trg_mask)\n",
    "        trg = self.self_attn_layer_norm_1(trg + self.dropout(_trg))\n",
    "\n",
    "        _trg, attention_score = self.self_attention_2(trg, enc_src, enc_src, src_mask)\n",
    "        trg = self.self_attn_layer_norm_2(trg + self.dropout(_trg))\n",
    "\n",
    "        _trg = self.feedforward(trg)\n",
    "        trg = self.feedforward(trg + self.dropout(_trg))\n",
    "\n",
    "        return trg, attention_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "601a9b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hidden_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hidden_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])  \n",
    "\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(device)\n",
    "\n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        # trg: [batch_size, trg_len, hideen_dim]\n",
    "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
    "\n",
    "        output = self.fc_out(trg)\n",
    "\n",
    "        # output: [batch_size, trg_len, output_dim]\n",
    "\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863c269b",
   "metadata": {},
   "source": [
    "### 트랜스포머 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b338705",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        # src: [batch_size, src_len]\n",
    "\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # src_mask = [batch_size, 1, 1, src_len]\n",
    "\n",
    "        return src_mask\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "\n",
    "        # trg: [batch_size, trg_len]\n",
    "\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=self.device)).bool()\n",
    "\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "\n",
    "        # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
    "\n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "\n",
    "        # src: [batch_size, src_len]\n",
    "        # trg: [batch_size, trg_len]\n",
    "\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "\n",
    "        # src_mask: [batch_size, 1, 1, src_len]\n",
    "        # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
    "\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "\n",
    "        # enc_src: [batch_size, src_len, hidden_dim]\n",
    "\n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        # output: [batch_size, trg_len, output_dim]\n",
    "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
    "\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae59e4d4",
   "metadata": {},
   "source": [
    "### 학습 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23efb89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab_src)\n",
    "OUTPUT_DIM = len(vocab_trg)\n",
    "HIDDEN_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebb24349",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_src.vocab[\"<pad>\"]\n",
    "\n",
    "SRC_PAD_IDX = vocab_src.vocab[\"<pad>\"]\n",
    "TRG_PAD_IDX = vocab_trg.vocab[\"<pad>\"]\n",
    "\n",
    "# 인코더(encoder)와 디코더(decoder) 객체 선언\n",
    "enc = Encoder(INPUT_DIM, HIDDEN_DIM, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, device)\n",
    "dec = Decoder(OUTPUT_DIM, HIDDEN_DIM, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, device)\n",
    "\n",
    "# Transformer 객체 선언\n",
    "model = Transformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19279a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 9,024,517 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144e4ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(7853, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttentionLayer(\n",
       "          (q_linear): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (k_linear): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (v_linear): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (out_linear): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): FeedForwardLayer(\n",
       "          (linear_1): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (linear_2): Linear(in_features=512, out_features=256, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(5893, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x DecoderLayer(\n",
       "        (self_attention_1): MultiheadAttentionLayer(\n",
       "          (q_linear): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (k_linear): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (v_linear): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (out_linear): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (self_attn_layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention_2): MultiheadAttentionLayer(\n",
       "          (q_linear): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (k_linear): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (v_linear): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (out_linear): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (self_attn_layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (feedforward): FeedForwardLayer(\n",
       "          (linear_1): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (linear_2): Linear(in_features=512, out_features=256, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ef12603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Adam optimizer로 학습 최적화\n",
    "LEARNING_RATE = 0.0005\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# 뒷 부분의 패딩(padding)에 대해서는 값 무시\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7b20233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train() # 학습 모드\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # 전체 학습 데이터를 확인하며\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch[0]\n",
    "        trg = batch[1]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 출력 단어의 마지막 인덱스()는 제외\n",
    "        # 입력을 할 때는 부터 시작하도록 처리\n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "\n",
    "        # output: [배치 크기, trg_len - 1, output_dim]\n",
    "        # trg: [배치 크기, trg_len]\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        # 출력 단어의 인덱스 0()은 제외\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "\n",
    "        # output: [배치 크기 * trg_len - 1, output_dim]\n",
    "        # trg: [배치 크기 * trg len - 1]\n",
    "\n",
    "        # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward() # 기울기(gradient) 계산\n",
    "\n",
    "        # 기울기(gradient) clipping 진행\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        # 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "        # 전체 손실 값 계산\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28f64219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval() # 평가 모드\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 전체 평가 데이터를 확인하며\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch[0]\n",
    "            trg = batch[1]\n",
    "\n",
    "            # 출력 단어의 마지막 인덱스()는 제외\n",
    "            # 입력을 할 때는 부터 시작하도록 처리\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "\n",
    "            # output: [배치 크기, trg_len - 1, output_dim]\n",
    "            # trg: [배치 크기, trg_len]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            # 출력 단어의 인덱스 0()은 제외\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "\n",
    "            # output: [배치 크기 * trg_len - 1, output_dim]\n",
    "            # trg: [배치 크기 * trg len - 1]\n",
    "\n",
    "            # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            # 전체 손실 값 계산\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bc8c0f",
   "metadata": {},
   "source": [
    "- 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ddfcc789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35b3a48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 10s\n",
      "\tTrain Loss: 4.312 | Train PPL: 74.599\n",
      "\tValidation Loss: 3.170 | Validation PPL: 23.797\n",
      "Epoch: 02 | Time: 0m 9s\n",
      "\tTrain Loss: 2.881 | Train PPL: 17.837\n",
      "\tValidation Loss: 2.440 | Validation PPL: 11.478\n",
      "Epoch: 03 | Time: 0m 9s\n",
      "\tTrain Loss: 2.331 | Train PPL: 10.290\n",
      "\tValidation Loss: 2.150 | Validation PPL: 8.588\n",
      "Epoch: 04 | Time: 0m 9s\n",
      "\tTrain Loss: 2.019 | Train PPL: 7.529\n",
      "\tValidation Loss: 2.005 | Validation PPL: 7.428\n",
      "Epoch: 05 | Time: 0m 9s\n",
      "\tTrain Loss: 1.813 | Train PPL: 6.127\n",
      "\tValidation Loss: 1.922 | Validation PPL: 6.834\n",
      "Epoch: 06 | Time: 0m 9s\n",
      "\tTrain Loss: 1.660 | Train PPL: 5.258\n",
      "\tValidation Loss: 1.894 | Validation PPL: 6.644\n",
      "Epoch: 07 | Time: 0m 9s\n",
      "\tTrain Loss: 1.539 | Train PPL: 4.660\n",
      "\tValidation Loss: 1.901 | Validation PPL: 6.692\n",
      "Epoch: 08 | Time: 0m 9s\n",
      "\tTrain Loss: 1.443 | Train PPL: 4.234\n",
      "\tValidation Loss: 1.881 | Validation PPL: 6.562\n",
      "Epoch: 09 | Time: 0m 10s\n",
      "\tTrain Loss: 1.363 | Train PPL: 3.907\n",
      "\tValidation Loss: 1.881 | Validation PPL: 6.560\n",
      "Epoch: 10 | Time: 0m 9s\n",
      "\tTrain Loss: 1.291 | Train PPL: 3.637\n",
      "\tValidation Loss: 1.877 | Validation PPL: 6.532\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time() # 시작 시간 기록\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion)\n",
    "\n",
    "    end_time = time.time() # 종료 시간 기록\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'transformer_german_to_english.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValidation Loss: {valid_loss:.3f} | Validation PPL: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13324a6",
   "metadata": {},
   "source": [
    "### 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5c93c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.919 | Test PPL: 6.812\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('transformer_german_to_english.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_loader, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9eb4d7d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<pad>',\n",
       " '<sos>',\n",
       " '<eos>',\n",
       " '.',\n",
       " 'ein',\n",
       " 'einem',\n",
       " 'in',\n",
       " 'eine',\n",
       " ',',\n",
       " 'und',\n",
       " 'mit',\n",
       " 'auf',\n",
       " 'mann',\n",
       " 'einer',\n",
       " 'der',\n",
       " 'frau',\n",
       " 'die',\n",
       " 'zwei',\n",
       " 'einen',\n",
       " 'im',\n",
       " 'an',\n",
       " 'von',\n",
       " 'sich',\n",
       " 'dem',\n",
       " 'mädchen',\n",
       " 'junge',\n",
       " 'vor',\n",
       " 'zu',\n",
       " 'steht',\n",
       " 'männer',\n",
       " 'sitzt',\n",
       " 'hund',\n",
       " 'den',\n",
       " 'straße',\n",
       " 'während',\n",
       " 'gruppe',\n",
       " 'hält',\n",
       " 'spielt',\n",
       " 'das',\n",
       " 'hemd',\n",
       " 'personen',\n",
       " 'über',\n",
       " 'drei',\n",
       " 'eines',\n",
       " 'frauen',\n",
       " 'blauen',\n",
       " 'neben',\n",
       " 'ist',\n",
       " 'kind',\n",
       " 'roten',\n",
       " 'weißen',\n",
       " 'stehen',\n",
       " 'sitzen',\n",
       " 'menschen',\n",
       " 'am',\n",
       " 'aus',\n",
       " 'spielen',\n",
       " 'durch',\n",
       " 'bei',\n",
       " 'geht',\n",
       " 'trägt',\n",
       " 'fährt',\n",
       " 'wasser',\n",
       " 'um',\n",
       " 'kinder',\n",
       " 'kleines',\n",
       " 'person',\n",
       " 'macht',\n",
       " 'springt',\n",
       " 'kleiner',\n",
       " 'schwarzen',\n",
       " 'entlang',\n",
       " 'leute',\n",
       " 'gehen',\n",
       " 'etwas',\n",
       " 'mehrere',\n",
       " 'seinem',\n",
       " 'großen',\n",
       " 'oberteil',\n",
       " 'jungen',\n",
       " 'hand',\n",
       " 'grünen',\n",
       " 'läuft',\n",
       " 'sind',\n",
       " 'für',\n",
       " 'hintergrund',\n",
       " 'fahrrad',\n",
       " 'freien',\n",
       " 'jacke',\n",
       " 'luft',\n",
       " 'strand',\n",
       " 'ball',\n",
       " 'hat',\n",
       " 'anderen',\n",
       " 'schaut',\n",
       " 'junger',\n",
       " 'kleidung',\n",
       " 'hinter',\n",
       " 'sie',\n",
       " 'nach',\n",
       " 'andere',\n",
       " 'gelben',\n",
       " 'kleine',\n",
       " 'gebäude',\n",
       " 'vier',\n",
       " 'hut',\n",
       " 'tisch',\n",
       " 'wird',\n",
       " 'beim',\n",
       " 'nähe',\n",
       " 'essen',\n",
       " 'kleinen',\n",
       " 'menschenmenge',\n",
       " 'schwarzer',\n",
       " 'kamera',\n",
       " 'paar',\n",
       " 'vorbei',\n",
       " 'gras',\n",
       " 'hoch',\n",
       " 't-shirt',\n",
       " 'hunde',\n",
       " 'boden',\n",
       " 'schnee',\n",
       " 'rennt',\n",
       " 'ihre',\n",
       " 'unter',\n",
       " 'ihren',\n",
       " 'kleid',\n",
       " 'gitarre',\n",
       " 'älterer',\n",
       " 'draußen',\n",
       " 'sein',\n",
       " 'zusammen',\n",
       " 'ihr',\n",
       " 'feld',\n",
       " 'bühne',\n",
       " 'fahren',\n",
       " 'brille',\n",
       " 'seine',\n",
       " 'herum',\n",
       " 'rennen',\n",
       " 'gehweg',\n",
       " 'kopf',\n",
       " 'sehen',\n",
       " 'er',\n",
       " 'blickt',\n",
       " 'dabei',\n",
       " 'foto',\n",
       " 'seinen',\n",
       " 'weißer',\n",
       " 'des',\n",
       " 'stadt',\n",
       " 'braunen',\n",
       " 'hinunter',\n",
       " 'park',\n",
       " 'liegt',\n",
       " 'laufen',\n",
       " 'versucht',\n",
       " 'arbeitet',\n",
       " 'jeans',\n",
       " 'als',\n",
       " 'bank',\n",
       " 'tragen',\n",
       " 'baby',\n",
       " 'dame',\n",
       " 'wie',\n",
       " 'machen',\n",
       " 'große',\n",
       " 'lächelt',\n",
       " 'schauen',\n",
       " 'brauner',\n",
       " 'ihrem',\n",
       " 'schwarz',\n",
       " 'ältere',\n",
       " 'orangefarbenen',\n",
       " 'ihm',\n",
       " 'junges',\n",
       " 'bürgersteig',\n",
       " 'arbeiten',\n",
       " 'hose',\n",
       " 'zeigt',\n",
       " 'sieht',\n",
       " 'spricht',\n",
       " 'liest',\n",
       " 'unterhalten',\n",
       " 'skateboard',\n",
       " 'halten',\n",
       " 'haaren',\n",
       " 'wand',\n",
       " 'weg',\n",
       " 'viele',\n",
       " 'sonnenbrille',\n",
       " 'zum',\n",
       " 'boot',\n",
       " 'pferd',\n",
       " 'gekleidete',\n",
       " 'seiner',\n",
       " 'hosen',\n",
       " 'rosa',\n",
       " 'tag',\n",
       " 'helm',\n",
       " 'weißem',\n",
       " 'grauen',\n",
       " 'singt',\n",
       " 'ihnen',\n",
       " 'blauem',\n",
       " 'gesicht',\n",
       " '„',\n",
       " 'bild',\n",
       " 'mantel',\n",
       " 'auto',\n",
       " 'posiert',\n",
       " 'darauf',\n",
       " 'felsen',\n",
       " 'fußball',\n",
       " 'fünf',\n",
       " 'klettert',\n",
       " 'führt',\n",
       " 'shorts',\n",
       " 'befindet',\n",
       " 'einige',\n",
       " 'sand',\n",
       " 'mikrofon',\n",
       " 'baum',\n",
       " 'blauer',\n",
       " 'schwarzem',\n",
       " 'sehr',\n",
       " 'wirft',\n",
       " 'gekleideter',\n",
       " 'band',\n",
       " 'es',\n",
       " 'weiß',\n",
       " '“',\n",
       " 'blau',\n",
       " 'meer',\n",
       " 'oder',\n",
       " 'rotem',\n",
       " 'isst',\n",
       " 'restaurant',\n",
       " 'wald',\n",
       " 'arbeiter',\n",
       " 'männern',\n",
       " 'schwarze',\n",
       " 'anderer',\n",
       " 'kindern',\n",
       " 'raum',\n",
       " 'wiese',\n",
       " 'fenster',\n",
       " 'langen',\n",
       " 'pullover',\n",
       " 'roter',\n",
       " 'wartet',\n",
       " 'weiße',\n",
       " 'ins',\n",
       " 'anzug',\n",
       " 'bluse',\n",
       " 'gestreiften',\n",
       " 'posieren',\n",
       " 'motorrad',\n",
       " 'haar',\n",
       " 'springen',\n",
       " 'tanzen',\n",
       " 'trikot',\n",
       " 'werden',\n",
       " 'oberkörper',\n",
       " 'voller',\n",
       " 'bereitet',\n",
       " 'stuhl',\n",
       " 'spieler',\n",
       " 'betrachtet',\n",
       " 'rot',\n",
       " 'alter',\n",
       " 'hügel',\n",
       " 'blonde',\n",
       " 'schild',\n",
       " 'zwischen',\n",
       " 'handy',\n",
       " 'oben',\n",
       " 'warten',\n",
       " 'leuten',\n",
       " 'schläft',\n",
       " 'berg',\n",
       " 'zieht',\n",
       " 'ab',\n",
       " 'mütze',\n",
       " 'buch',\n",
       " 'lehnt',\n",
       " 'reitet',\n",
       " 'händen',\n",
       " 'beobachtet',\n",
       " 'maul',\n",
       " 'miteinander',\n",
       " 'stock',\n",
       " 'bäumen',\n",
       " 'fluss',\n",
       " 'ihrer',\n",
       " 'tanzt',\n",
       " 'bauarbeiter',\n",
       " 'kleinkind',\n",
       " 'rucksack',\n",
       " 'arm',\n",
       " 'lächeln',\n",
       " 'schiebt',\n",
       " 'vom',\n",
       " 'befinden',\n",
       " 'gegen',\n",
       " 'haben',\n",
       " 'radfahrer',\n",
       " 'küche',\n",
       " 'see',\n",
       " 'seite',\n",
       " 'spielzeug',\n",
       " 'kurzen',\n",
       " 'schwimmbecken',\n",
       " 'asiatische',\n",
       " 'welle',\n",
       " 'rücken',\n",
       " 'spiel',\n",
       " 'treppe',\n",
       " 'zaun',\n",
       " 'verkauft',\n",
       " 'zur',\n",
       " 'familie',\n",
       " 'fangen',\n",
       " 'jemand',\n",
       " 'mitten',\n",
       " 'trinkt',\n",
       " 'versammelt',\n",
       " 'großer',\n",
       " 'erwachsene',\n",
       " 'rosafarbenen',\n",
       " 'blumen',\n",
       " 'hängt',\n",
       " 'ihn',\n",
       " 'schneidet',\n",
       " 'geschäft',\n",
       " 'gerade',\n",
       " 'markt',\n",
       " 'telefoniert',\n",
       " 'belebten',\n",
       " 'bunten',\n",
       " 'malt',\n",
       " 'spazieren',\n",
       " 'umgeben',\n",
       " 'nimmt',\n",
       " 'reihe',\n",
       " 'blicken',\n",
       " 'gewässer',\n",
       " 'rote',\n",
       " 'schwimmt',\n",
       " 'zug',\n",
       " 'autos',\n",
       " 'bus',\n",
       " 'denen',\n",
       " 'freiem',\n",
       " 'schneebedeckten',\n",
       " 'vielen',\n",
       " 'gekleidet',\n",
       " 'schlägt',\n",
       " 'brücke',\n",
       " 'fuß',\n",
       " 'gebäudes',\n",
       " 'hände',\n",
       " 'mutter',\n",
       " 'sonnigen',\n",
       " 'wagen',\n",
       " 'fliegt',\n",
       " 'himmel',\n",
       " 'publikum',\n",
       " 'bart',\n",
       " 'fußballspieler',\n",
       " 'hebt',\n",
       " 'kämpfen',\n",
       " 'richtung',\n",
       " 'tasche',\n",
       " 'dunklen',\n",
       " 'hemden',\n",
       " 'herunter',\n",
       " 'geländer',\n",
       " 'grünem',\n",
       " 'weste',\n",
       " 'hohen',\n",
       " 'platz',\n",
       " 'asiatischer',\n",
       " 'blondes',\n",
       " 'graffiti',\n",
       " 'haus',\n",
       " 'weißes',\n",
       " 'zuschauer',\n",
       " 'alte',\n",
       " 'kopfbedeckung',\n",
       " 'seil',\n",
       " 'zeitung',\n",
       " 'überqueren',\n",
       " 'mannes',\n",
       " 'hinauf',\n",
       " 'sprung',\n",
       " 'teil',\n",
       " 'zuschauen',\n",
       " 'bereiten',\n",
       " 'musik',\n",
       " 'tritt',\n",
       " 'davon',\n",
       " 'genießt',\n",
       " 'orangen',\n",
       " 'basketball',\n",
       " 'blaue',\n",
       " 'decke',\n",
       " 'hof',\n",
       " 'karierten',\n",
       " 'lacht',\n",
       " 'mittleren',\n",
       " 'sechs',\n",
       " 'shirt',\n",
       " 'benutzt',\n",
       " 'da',\n",
       " 'schirm',\n",
       " 'zuschauern',\n",
       " 'alten',\n",
       " 'fest',\n",
       " 'kunststück',\n",
       " 'mauer',\n",
       " 'menge',\n",
       " 'papier',\n",
       " 'stück',\n",
       " 'einigen',\n",
       " 'großes',\n",
       " 'unterhält',\n",
       " 'alters',\n",
       " 'blick',\n",
       " 'mensch',\n",
       " 'rasen',\n",
       " 'spielplatz',\n",
       " 'zusieht',\n",
       " 'baseballspieler',\n",
       " 'mitte',\n",
       " 'nahe',\n",
       " 'nebeneinander',\n",
       " 'schaukel',\n",
       " 'sofa',\n",
       " 'zuschaut',\n",
       " 'armen',\n",
       " 'lässt',\n",
       " 'nachts',\n",
       " 'trinken',\n",
       " '–',\n",
       " 'bereit',\n",
       " 'fotos',\n",
       " 'lila',\n",
       " 'pool',\n",
       " 'rock',\n",
       " 'typ',\n",
       " 'zusehen',\n",
       " 'augen',\n",
       " 'dach',\n",
       " 'gegenstand',\n",
       " 'gelber',\n",
       " 'hinten',\n",
       " 'versuchen',\n",
       " 'zigarette',\n",
       " 'bier',\n",
       " 'gibt',\n",
       " 'mehreren',\n",
       " 'scheint',\n",
       " 'sonne',\n",
       " 'beobachten',\n",
       " 'mund',\n",
       " 'surfer',\n",
       " 'treten',\n",
       " 'beide',\n",
       " 'flagge',\n",
       " 'fängt',\n",
       " 'kniet',\n",
       " 'nummer',\n",
       " 'outfit',\n",
       " 'raucht',\n",
       " 'schal',\n",
       " 'uniform',\n",
       " 'öffentlichen',\n",
       " 'betrachten',\n",
       " 'ferne',\n",
       " 'nicht',\n",
       " 'schürze',\n",
       " 'stange',\n",
       " 'stufen',\n",
       " 'trikots',\n",
       " 'verschneiten',\n",
       " 'führen',\n",
       " 'liegen',\n",
       " 'wanderer',\n",
       " 'blonden',\n",
       " 'diese',\n",
       " 'erwachsener',\n",
       " 'frisbee',\n",
       " 'gehsteig',\n",
       " 'rotes',\n",
       " 'außerhalb',\n",
       " 'einander',\n",
       " 'holz',\n",
       " 'lachen',\n",
       " 'rampe',\n",
       " 'snowboarder',\n",
       " 'teenager',\n",
       " 'tennisball',\n",
       " 'unten',\n",
       " 'gemüse',\n",
       " 'u-bahn',\n",
       " 'bett',\n",
       " 'greift',\n",
       " 'männliche',\n",
       " 'reden',\n",
       " 'rutscht',\n",
       " 'veranstaltung',\n",
       " 'aufschrift',\n",
       " 'braune',\n",
       " 'bärtiger',\n",
       " 'fotografiert',\n",
       " 'grün',\n",
       " 'hilft',\n",
       " 'kleidern',\n",
       " 'nacht',\n",
       " 'schwarz-weißer',\n",
       " 'beiden',\n",
       " 'hüten',\n",
       " 'sonnenuntergang',\n",
       " 'surft',\n",
       " 'werfen',\n",
       " 'dass',\n",
       " 'gelbe',\n",
       " 'gelbem',\n",
       " 'grüner',\n",
       " 'haare',\n",
       " 'herr',\n",
       " 'kostümen',\n",
       " 'leiter',\n",
       " 'man',\n",
       " 'musiker',\n",
       " 'skifahrer',\n",
       " 'überquert',\n",
       " 'art',\n",
       " 'damen',\n",
       " 'legt',\n",
       " 'nacktem',\n",
       " 'polizisten',\n",
       " 'statue',\n",
       " 'städtischen',\n",
       " 'ufer',\n",
       " 'versammeln',\n",
       " 'zelt',\n",
       " 'basketballspieler',\n",
       " 'blonder',\n",
       " 'erwachsenen',\n",
       " 'maschine',\n",
       " 'nehmen',\n",
       " 'orange',\n",
       " 'tür',\n",
       " 'bar',\n",
       " 'blaues',\n",
       " 'kommt',\n",
       " 'leine',\n",
       " 'mannschaft',\n",
       " 'rutsche',\n",
       " 'tor',\n",
       " 'brunnen',\n",
       " 'genießen',\n",
       " 'guckt',\n",
       " 'pfad',\n",
       " 'polizist',\n",
       " 'schreibt',\n",
       " 'stellt',\n",
       " 'sweatshirt',\n",
       " 'vater',\n",
       " 'westen',\n",
       " 'asiatischen',\n",
       " 'eimer',\n",
       " 'fisch',\n",
       " 'gelb',\n",
       " 'leeren',\n",
       " 'lesen',\n",
       " 'läufer',\n",
       " 'stühlen',\n",
       " 'braut',\n",
       " 'ende',\n",
       " 'feuer',\n",
       " 'gekleideten',\n",
       " 'gestreiftem',\n",
       " 'getränk',\n",
       " 'grill',\n",
       " 'her',\n",
       " 'inmitten',\n",
       " 'kaufen',\n",
       " 'klettern',\n",
       " 'obst',\n",
       " 'ohne',\n",
       " 'regen',\n",
       " 'schlagzeug',\n",
       " 'sprechen',\n",
       " 'streckt',\n",
       " 'umgebung',\n",
       " 'verschiedene',\n",
       " 'wobei',\n",
       " 'badeanzug',\n",
       " 'eis',\n",
       " 'fahrzeug',\n",
       " 'riesigen',\n",
       " 'schuhen',\n",
       " 'schutzhelm',\n",
       " 'schwingt',\n",
       " 'wellen',\n",
       " 'anderes',\n",
       " 'arme',\n",
       " 'gegend',\n",
       " 'gelbes',\n",
       " 'gemeinsam',\n",
       " 'grauem',\n",
       " 'handschuhen',\n",
       " 'instrument',\n",
       " 'motorradfahrer',\n",
       " 'parade',\n",
       " 'parkplatz',\n",
       " 'regenschirm',\n",
       " 'schlagen',\n",
       " 'singen',\n",
       " 'sohn',\n",
       " 'soldaten',\n",
       " 'trick',\n",
       " 'uniformen',\n",
       " 'älteren',\n",
       " 'bergen',\n",
       " 'gebäuden',\n",
       " 'hört',\n",
       " 'jugendlicher',\n",
       " 'kinderwagen',\n",
       " 'korb',\n",
       " 'offenen',\n",
       " 'schwarzes',\n",
       " 'starrt',\n",
       " 'straßenecke',\n",
       " 'straßenrand',\n",
       " '\\xa0',\n",
       " 'arbeit',\n",
       " 'baustelle',\n",
       " 'bereich',\n",
       " 'dieser',\n",
       " 'jagt',\n",
       " 'lächelnd',\n",
       " 'mikrophon',\n",
       " 'pinkfarbenen',\n",
       " 'schoß',\n",
       " 'straßen',\n",
       " 'umarmt',\n",
       " 'zurück',\n",
       " 'computer',\n",
       " 'dunkelhäutiger',\n",
       " 'gleich',\n",
       " 'grüne',\n",
       " 'jacken',\n",
       " 'kostüm',\n",
       " 'krawatte',\n",
       " 'laden',\n",
       " 'orangefarbener',\n",
       " 'pause',\n",
       " 'waren',\n",
       " ' ',\n",
       " '\"',\n",
       " 'kurz',\n",
       " 'repariert',\n",
       " 'schreibtisch',\n",
       " 'volleyball',\n",
       " 'vordergrund',\n",
       " 'alle',\n",
       " 'baseball',\n",
       " 'ganz',\n",
       " 'garten',\n",
       " 'gegeneinander',\n",
       " 'gekleidetes',\n",
       " 'gut',\n",
       " 'konzert',\n",
       " 'kuchen',\n",
       " 'kurzer',\n",
       " 'putzt',\n",
       " 'schatten',\n",
       " 'vogel',\n",
       " 'asiatisches',\n",
       " 'balanciert',\n",
       " 'bedeckt',\n",
       " 'fahrradfahrer',\n",
       " 'fahrrädern',\n",
       " 'grünes',\n",
       " 'jemandem',\n",
       " 'kunden',\n",
       " 'lächelnde',\n",
       " 'pflanzen',\n",
       " 'sandalen',\n",
       " 'verkaufen',\n",
       " 'beugt',\n",
       " 'bikini',\n",
       " 'erde',\n",
       " 'fell',\n",
       " 'gegenüber',\n",
       " 'land',\n",
       " 'lkw',\n",
       " 'netz',\n",
       " 'party',\n",
       " 'pferden',\n",
       " 'setzt',\n",
       " 'stein',\n",
       " 'stiefeln',\n",
       " 'weibliche',\n",
       " 'winkt',\n",
       " 'ziehen',\n",
       " 'dreht',\n",
       " 'einkaufswagen',\n",
       " 'felswand',\n",
       " 'football',\n",
       " 'football-spieler',\n",
       " 'footballspieler',\n",
       " 'fällt',\n",
       " 'glas',\n",
       " 'halsband',\n",
       " 'klassenzimmer',\n",
       " 'mobiltelefon',\n",
       " 'orangefarbenem',\n",
       " 'reiten',\n",
       " 'schiedsrichter',\n",
       " 'schulter',\n",
       " 'schüler',\n",
       " 'snowboard',\n",
       " 'surfbrett',\n",
       " 'wo',\n",
       " 'badehose',\n",
       " 'belebte',\n",
       " 'dessen',\n",
       " 'hellbrauner',\n",
       " 'höhe',\n",
       " 'schlitten',\n",
       " 'schläger',\n",
       " 'team',\n",
       " 'unbefestigten',\n",
       " 'aussieht',\n",
       " 'bein',\n",
       " 'braunem',\n",
       " 'davor',\n",
       " 'durchs',\n",
       " 'kajak',\n",
       " 'kanu',\n",
       " 'leuchtend',\n",
       " 'oberteilen',\n",
       " 'rad',\n",
       " 'redet',\n",
       " 'schuhe',\n",
       " 'schwimmen',\n",
       " 'seines',\n",
       " 'stand',\n",
       " 'was',\n",
       " 'zeigen',\n",
       " 'ecke',\n",
       " 'küssen',\n",
       " 'küsst',\n",
       " 'loch',\n",
       " 'schlange',\n",
       " 'schnauze',\n",
       " 'spiegel',\n",
       " 'tischen',\n",
       " 'verschiedenen',\n",
       " 'asiatisch',\n",
       " 'base',\n",
       " 'bäume',\n",
       " 'farbenfrohen',\n",
       " 'grasbewachsenen',\n",
       " 'hin',\n",
       " 'kreis',\n",
       " 'künstler',\n",
       " 'laptop',\n",
       " 'lilafarbenen',\n",
       " 'mikroskop',\n",
       " 'männlicher',\n",
       " 'rennstrecke',\n",
       " 'schultern',\n",
       " 'schönen',\n",
       " 'seifenblasen',\n",
       " 'skateboarder',\n",
       " 'spaß',\n",
       " 't-shirts',\n",
       " 'taschen',\n",
       " 'telefon',\n",
       " 'tochter',\n",
       " 'akkordeon',\n",
       " 'flasche',\n",
       " 'gepflasterten',\n",
       " 'gerüst',\n",
       " 'haufen',\n",
       " 'kai',\n",
       " 'kirche',\n",
       " 'kocht',\n",
       " 'spielfeld',\n",
       " 'tanktop',\n",
       " 'tasse',\n",
       " 'zimmer',\n",
       " 'aussehende',\n",
       " 'bedient',\n",
       " 'bemalt',\n",
       " 'café',\n",
       " 'farben',\n",
       " 'freunde',\n",
       " 'freunden',\n",
       " 'gegnerischen',\n",
       " 'hellen',\n",
       " 'hunden',\n",
       " 'küste',\n",
       " 'nur',\n",
       " 'parkbank',\n",
       " 'rodeo',\n",
       " 'schaukelt',\n",
       " 'tennis',\n",
       " 'unterwegs',\n",
       " 'beleuchteten',\n",
       " 'bis',\n",
       " 'braun-weißer',\n",
       " 'damit',\n",
       " 'entspannt',\n",
       " 'fleisch',\n",
       " 'gasse',\n",
       " 'geige',\n",
       " 'grauer',\n",
       " 'helmen',\n",
       " 'kappe',\n",
       " 'katze',\n",
       " 'sachen',\n",
       " 'schaufel',\n",
       " 'schutzhelmen',\n",
       " 'schwimmbrille',\n",
       " 'sieben',\n",
       " 'ski',\n",
       " 'stadion',\n",
       " 'steine',\n",
       " 'teich',\n",
       " 'trampolin',\n",
       " 'umarmen',\n",
       " 'violetten',\n",
       " 'berge',\n",
       " 'cowboy',\n",
       " 'hang',\n",
       " 'holt',\n",
       " 'hüpft',\n",
       " 'jongliert',\n",
       " 'kopfhörer',\n",
       " 'langem',\n",
       " 'maske',\n",
       " 'overall',\n",
       " 'rand',\n",
       " 'schlamm',\n",
       " 'schüssel',\n",
       " 'skiern',\n",
       " 'so',\n",
       " 'steigt',\n",
       " 'strecke',\n",
       " 'tänzer',\n",
       " 'verwendet',\n",
       " 'voll',\n",
       " ';',\n",
       " 'barfuß',\n",
       " 'braun',\n",
       " 'couch',\n",
       " 'dunkler',\n",
       " 'feldweg',\n",
       " 'flaggen',\n",
       " 'fotografieren',\n",
       " 'fußballspiel',\n",
       " 'kaffee',\n",
       " 'kauft',\n",
       " 'klippe',\n",
       " 'landschaft',\n",
       " 'lastwagen',\n",
       " 'links',\n",
       " 'lächelnder',\n",
       " 'rechts',\n",
       " 'reifen',\n",
       " 'reparieren',\n",
       " 'roller',\n",
       " 'schießt',\n",
       " 'tanz',\n",
       " 'anzügen',\n",
       " 'asiate',\n",
       " 'aussehender',\n",
       " 'bekommt',\n",
       " 'berührt',\n",
       " 'bläst',\n",
       " 'cowboyhut',\n",
       " 'dunkelhäutige',\n",
       " 'fahnen',\n",
       " 'grillt',\n",
       " 'heben',\n",
       " 'hellblauen',\n",
       " 'hockt',\n",
       " 'instrumenten',\n",
       " 'kopftuch',\n",
       " 'objekt',\n",
       " 'ringen',\n",
       " 'ruht',\n",
       " 'schüttelt',\n",
       " 'sitzende',\n",
       " 'teller',\n",
       " 'verfolgt',\n",
       " 'wandert',\n",
       " 'wüste',\n",
       " 'beinen',\n",
       " 'bilder',\n",
       " 'clown',\n",
       " 'flugzeug',\n",
       " 'geschlossenen',\n",
       " 'handtasche',\n",
       " 'hängen',\n",
       " 'instrumente',\n",
       " 'linken',\n",
       " 'mahlzeit',\n",
       " 'metall',\n",
       " 'nase',\n",
       " 'new',\n",
       " 'personengruppe',\n",
       " 'reinigt',\n",
       " 'springbrunnen',\n",
       " 'veranda',\n",
       " 'überfüllten',\n",
       " 'asiaten',\n",
       " 'asiatin',\n",
       " 'auch',\n",
       " 'bauen',\n",
       " 'blatt',\n",
       " 'diesem',\n",
       " 'felsigen',\n",
       " 'geländemotorrad',\n",
       " 'gezogen',\n",
       " 'gucken',\n",
       " 'hauses',\n",
       " 'heraus',\n",
       " 'hinab',\n",
       " 'marathon',\n",
       " 'müll',\n",
       " 'pinkfarbenem',\n",
       " 'salto',\n",
       " 'tennisspieler',\n",
       " 'traditioneller',\n",
       " 'öffnet',\n",
       " 'üben',\n",
       " 'auftritt',\n",
       " 'balkon',\n",
       " 'dieses',\n",
       " 'fahrräder',\n",
       " 'kann',\n",
       " 'kerl',\n",
       " 'lederjacke',\n",
       " 'nachdem',\n",
       " 'reiter',\n",
       " 'rothaarige',\n",
       " 'schneiden',\n",
       " 'schnell',\n",
       " 'steinen',\n",
       " 'sänger',\n",
       " 'taucht',\n",
       " 'umzug',\n",
       " 'untersucht',\n",
       " 'winterkleidung',\n",
       " 'wäscht',\n",
       " 'zeit',\n",
       " 'allein',\n",
       " 'amerikanische',\n",
       " 'bach',\n",
       " 'bahn',\n",
       " 'beieinander',\n",
       " 'bunte',\n",
       " 'finger',\n",
       " 'gebiet',\n",
       " 'geld',\n",
       " 'gelegt',\n",
       " 'gießt',\n",
       " 'hellbraunen',\n",
       " 'kickt',\n",
       " 'kurve',\n",
       " 'passanten',\n",
       " 'pferde',\n",
       " 'schießen',\n",
       " 'schlafen',\n",
       " 'schlauch',\n",
       " 'sitzender',\n",
       " 'skateboardfahrer',\n",
       " 'teams',\n",
       " 'teppich',\n",
       " 'terrasse',\n",
       " 'tuch',\n",
       " 'vollen',\n",
       " 'weitere',\n",
       " 'wurde',\n",
       " 'wurf',\n",
       " 'zunge',\n",
       " 'zähne',\n",
       " 'übt',\n",
       " 'ausrüstung',\n",
       " 'beton',\n",
       " ...]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_src.get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fff96c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역(translation) 함수\n",
    "def translate_sentence(sentence, vocab_src, vocab_trg, model, device, max_len=50, logging=True):\n",
    "    model.eval() # 평가 모드\n",
    "\n",
    "    init_token = \"<sos>\"\n",
    "    eos_token = \"<eos>\"\n",
    "\n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('de_core_news_sm')\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    # 처음에  토큰, 마지막에  토큰 붙이기\n",
    "    tokens = [init_token] + tokens + [eos_token]\n",
    "    if logging:\n",
    "        print(f\"전체 소스 토큰: {tokens}\")\n",
    "\n",
    "    src_indexes = [vocab_src[token] for token in tokens]\n",
    "    if logging:\n",
    "        print(f\"소스 문장 인덱스: {src_indexes}\")\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "    # 소스 문장에 따른 마스크 생성\n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "\n",
    "    # 인코더(endocer)에 소스 문장을 넣어 출력 값 구하기\n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "    # 처음에는  토큰 하나만 가지고 있도록 하기\n",
    "    trg_indexes = [vocab_trg[init_token]]\n",
    "\n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "        # 출력 문장에 따른 마스크 생성\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        # 출력 문장에서 가장 마지막 단어만 사용\n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        trg_indexes.append(pred_token) # 출력 문장에 더하기\n",
    "        # 를 만나는 순간 끝\n",
    "        if pred_token == vocab_trg[eos_token]:\n",
    "            break\n",
    "\n",
    "    # 각 출력 단어 인덱스를 실제 단어로 변환\n",
    "    trg_tokens = [vocab_trg.get_itos()[i] for i in trg_indexes]\n",
    "\n",
    "    # 첫 번째 는 제외하고 출력 문장 반환\n",
    "    return trg_tokens[1:], attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0e9c4634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A man in an orange hat starring at something.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[\"en\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_src.get_itos()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6dcbddb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "소스 문장: Erzählen Sie mir die große Neuigkeit des Tages.\n",
      "타겟 문장: tensor([[  2,  16,  24,  ...,   1,   1,   1],\n",
      "        [  2, 113,  30,  ...,   1,   1,   1],\n",
      "        [  2,   4,  53,  ...,   1,   1,   1],\n",
      "        ...,\n",
      "        [  2,  16,  50,  ...,   1,   1,   1],\n",
      "        [  2,   4,  38,  ...,   1,   1,   1],\n",
      "        [  2,   4,  14,  ...,   1,   1,   1]], device='cuda:0')\n",
      "전체 소스 토큰: ['<sos>', 'erzählen', 'sie', 'mir', 'die', 'große', 'neuigkeit', 'des', 'tages', '.', '<eos>']\n",
      "소스 문장 인덱스: [2, 0, 99, 6783, 17, 168, 0, 151, 4140, 4, 3]\n",
      "모델 출력 결과: <unk> looking at the large <unk> of the daytime . <eos>\n"
     ]
    }
   ],
   "source": [
    "# example_idx = 23\n",
    "\n",
    "# src = test_dataset[\"de\"][example_idx]\n",
    "# trg = test_dataset[\"en\"][example_idx]\n",
    "\n",
    "src = \"Erzählen Sie mir die große Neuigkeit des Tages.\" # Tell me the big news of the day.\n",
    "\n",
    "print(f'소스 문장: {src}')\n",
    "print(f'타겟 문장: {trg}')\n",
    "\n",
    "translation, attention = translate_sentence(src, vocab_src, vocab_trg, model, device, logging=True)\n",
    "\n",
    "print(\"모델 출력 결과:\", \" \".join(translation))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
