{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 커스텀 OPT 및 Huggingface OPT 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers.models.opt.modeling_opt import OPTLearnedPositionalEmbedding\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_opt.opt import CustomOPTModel\n",
    "from custom_opt.load import load_hugginface_opt, load_custom_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomOPTModel(\n",
       "  (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
       "  (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x OPTDecoderLayer(\n",
       "      (self_attn): OPTAttention(\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (activation_fn): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "tokenizer_a, model_a = load_hugginface_opt(device=device)\n",
    "tokenizer_b, model_b = load_custom_opt(device=device)\n",
    "\n",
    "model_a.eval()\n",
    "model_b.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x OPTDecoderLayer(\n",
       "          (self_attn): OPTSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda\n",
      "model_a가 예측한 다음 토큰:  great\n",
      "model_b가 예측한 다음 토큰:  great\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. 토크나이저 준비 (OPT 기반)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\", use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # OPT는 pad_token을 eos_token으로 설정\n",
    "\n",
    "# 2. 입력 문장\n",
    "input_text = \"This is a\"\n",
    "\n",
    "# 3. 텍스트를 토크나이즈해서 텐서로 변환\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids  # shape: [1, seq_len]\n",
    "\n",
    "# 4. 두 모델 모두 eval 모드로 전환\n",
    "model_a.eval()\n",
    "model_b.eval()\n",
    "\n",
    "print(model_a.device)\n",
    "print(model_b.device)\n",
    "\n",
    "# 5. 추론\n",
    "with torch.no_grad():\n",
    "    logits_a = model_a(input_ids.to(model_a.device))  # device 맞추기\n",
    "    logits_b = model_b(input_ids.to(model_b.device))\n",
    "\n",
    "# 6. 각 모델에서 다음 토큰 예측\n",
    "next_token_id_a = torch.argmax(logits_a.logits[:, -1, :], dim=-1)  # 마지막 토큰 기준\n",
    "next_token_id_b = torch.argmax(logits_a.logits[:, -1, :], dim=-1)\n",
    "\n",
    "# 7. 디코딩해서 보기\n",
    "next_token_a = tokenizer.decode(next_token_id_a)\n",
    "next_token_b = tokenizer.decode(next_token_id_b)\n",
    "\n",
    "print(f\"model_a가 예측한 다음 토큰: {next_token_a}\")\n",
    "print(f\"model_b가 예측한 다음 토큰: {next_token_b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What happened to me?\n",
      "I'm not sure. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just\n"
     ]
    }
   ],
   "source": [
    "# 입력 문장\n",
    "input_text = \"What happened to me?\"\n",
    "\n",
    "# 텍스트를 토크나이징\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# 모델에 넣어 출력 생성 (greedy decoding: 가장 높은 확률 토큰을 선택)\n",
    "output = model_a.generate(\n",
    "    **inputs,\n",
    "    max_length=500,    # 최대 생성 길이 설정\n",
    "    do_sample=False,  # 무작위성 없이 가장 확률 높은 것 선택\n",
    ")\n",
    "\n",
    "# 출력 토큰을 다시 텍스트로 디코딩\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What happened to me?\n",
      "I'm not sure. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just me. I'm not sure if it's just me or if it's just\n"
     ]
    }
   ],
   "source": [
    "# 입력 문장\n",
    "input_text = \"What happened to me?\"\n",
    "\n",
    "# 텍스트를 토크나이징\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# 모델에 넣어 출력 생성 (greedy decoding: 가장 높은 확률 토큰을 선택)\n",
    "output = model_b.generate(\n",
    "    **inputs,\n",
    "    max_length=500,    # 최대 생성 길이 설정\n",
    "    do_sample=False,  # 무작위성 없이 가장 확률 높은 것 선택\n",
    ")\n",
    "\n",
    "# 출력 토큰을 다시 텍스트로 디코딩\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
